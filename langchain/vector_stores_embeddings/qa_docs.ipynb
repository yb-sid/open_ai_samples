{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv,find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "persist_directory = 'db/chroma/'\n",
    "embedding = OpenAIEmbeddings()\n",
    "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209\n"
     ]
    }
   ],
   "source": [
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What are major topics for this class?\"\n",
    "docs = vectordb.similarity_search(question,k=3) # this does not involve call to API\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "import langchain\n",
    "langchain.debug = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default , retreivalQA uses stuff : fit all relevant context into one chunk.\n",
    "\n",
    "But we divide using map_reduce , refine and map_rerank can be used to overcome context limit.\n",
    "But they will use more API calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    chain_type=\"stuff\", \n",
    "    retriever=vectordb.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"query\": \"What are major topics for this class?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain > 4:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What are major topics for this class?\",\n",
      "  \"context\": \"statistics for a while or maybe algebra, we'll go over those in the discussion sections as a \\nrefresher for those of you that want one.  \\nLater in this quarter, we'll also use the disc ussion sections to go over extensions for the \\nmaterial that I'm teaching in the main lectur es. So machine learning is a huge field, and \\nthere are a few extensions that we really want  to teach but didn't have time in the main \\nlectures for.\\n\\nstatistics for a while or maybe algebra, we'll go over those in the discussion sections as a \\nrefresher for those of you that want one.  \\nLater in this quarter, we'll also use the disc ussion sections to go over extensions for the \\nmaterial that I'm teaching in the main lectur es. So machine learning is a huge field, and \\nthere are a few extensions that we really want  to teach but didn't have time in the main \\nlectures for.\\n\\nmiddle of class, but because there won't be video you can safely sit there and make faces \\nat me, and that won't show, okay?  \\nLet's see. I also handed out this — ther e were two handouts I hope most of you have, \\ncourse information handout. So let me just sa y a few words about parts of these. On the \\nthird page, there's a section that says Online Resources.  \\nOh, okay. Louder? Actually, could you turn up the volume? Testing. Is this better? \\nTesting, testing. Okay, cool. Thanks.\\n\\nmiddle of class, but because there won't be video you can safely sit there and make faces \\nat me, and that won't show, okay?  \\nLet's see. I also handed out this — ther e were two handouts I hope most of you have, \\ncourse information handout. So let me just sa y a few words about parts of these. On the \\nthird page, there's a section that says Online Resources.  \\nOh, okay. Louder? Actually, could you turn up the volume? Testing. Is this better? \\nTesting, testing. Okay, cool. Thanks.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain > 4:chain:LLMChain > 5:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: Use the following pieces of context to answer the users question. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\nstatistics for a while or maybe algebra, we'll go over those in the discussion sections as a \\nrefresher for those of you that want one.  \\nLater in this quarter, we'll also use the disc ussion sections to go over extensions for the \\nmaterial that I'm teaching in the main lectur es. So machine learning is a huge field, and \\nthere are a few extensions that we really want  to teach but didn't have time in the main \\nlectures for.\\n\\nstatistics for a while or maybe algebra, we'll go over those in the discussion sections as a \\nrefresher for those of you that want one.  \\nLater in this quarter, we'll also use the disc ussion sections to go over extensions for the \\nmaterial that I'm teaching in the main lectur es. So machine learning is a huge field, and \\nthere are a few extensions that we really want  to teach but didn't have time in the main \\nlectures for.\\n\\nmiddle of class, but because there won't be video you can safely sit there and make faces \\nat me, and that won't show, okay?  \\nLet's see. I also handed out this — ther e were two handouts I hope most of you have, \\ncourse information handout. So let me just sa y a few words about parts of these. On the \\nthird page, there's a section that says Online Resources.  \\nOh, okay. Louder? Actually, could you turn up the volume? Testing. Is this better? \\nTesting, testing. Okay, cool. Thanks.\\n\\nmiddle of class, but because there won't be video you can safely sit there and make faces \\nat me, and that won't show, okay?  \\nLet's see. I also handed out this — ther e were two handouts I hope most of you have, \\ncourse information handout. So let me just sa y a few words about parts of these. On the \\nthird page, there's a section that says Online Resources.  \\nOh, okay. Louder? Actually, could you turn up the volume? Testing. Is this better? \\nTesting, testing. Okay, cool. Thanks.\\nHuman: What are major topics for this class?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain > 4:chain:LLMChain > 5:llm:ChatOpenAI] [1.12s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The major topics for this class are machine learning and its various extensions.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"The major topics for this class are machine learning and its various extensions.\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 507,\n",
      "      \"completion_tokens\": 14,\n",
      "      \"total_tokens\": 521\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain > 4:chain:LLMChain] [1.12s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"The major topics for this class are machine learning and its various extensions.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain] [1.12s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output_text\": \"The major topics for this class are machine learning and its various extensions.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA] [1.70s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"result\": \"The major topics for this class are machine learning and its various extensions.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "result = qa_chain({\"query\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The major topics for this class are machine learning and its various applications.\n"
     ]
    }
   ],
   "source": [
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define custom Prompt : this is the main peice of information given to LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Build prompt\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer. \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever(search_type=\"mmr\"),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Is statistics a class topic?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"query\": \"Is statistics a class topic?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain > 4:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Is statistics a class topic?\",\n",
      "  \"context\": \"statistics for a while or maybe algebra, we'll go over those in the discussion sections as a \\nrefresher for those of you that want one.  \\nLater in this quarter, we'll also use the disc ussion sections to go over extensions for the \\nmaterial that I'm teaching in the main lectur es. So machine learning is a huge field, and \\nthere are a few extensions that we really want  to teach but didn't have time in the main \\nlectures for.\\n\\nof this class will not be very program ming intensive, although we will do some \\nprogramming, mostly in either MATLAB or Octa ve. I'll say a bit more about that later.  \\nI also assume familiarity with basic proba bility and statistics. So most undergraduate \\nstatistics class, like Stat 116 taught here at Stanford, will be more than enough. I'm gonna \\nassume all of you know what ra ndom variables are, that all of you know what expectation \\nis, what a variance or a random variable is. And in case of some of you, it's been a while \\nsince you've seen some of this material. At some of the discussion sections, we'll actually \\ngo over some of the prerequisites, sort of as  a refresher course under prerequisite class. \\nI'll say a bit more about that later as well.  \\nLastly, I also assume familiarity with basi c linear algebra. And again, most undergraduate \\nlinear algebra courses are more than enough. So if you've taken courses like Math 51, \\n103, Math 113 or CS205 at Stanford, that would be more than enough. Basically, I'm \\ngonna assume that all of you know what matrix es and vectors are, that you know how to \\nmultiply matrices and vectors and multiply matrix and matrices, that you know what a matrix inverse is. If you know what an eigenvect or of a matrix is, that'd be even better. \\nBut if you don't quite know or if you're not qu ite sure, that's fine, too. We'll go over it in \\nthe review sections.\\n\\nmany biologers are there here? Wow, just a few, not many. I'm surprised. Anyone from \\nstatistics? Okay, a few. So where are the rest of you from?  \\nStudent : iCME.  \\nInstructor (Andrew Ng) : Say again?  \\nStudent : iCME.  \\nInstructor (Andrew Ng) : iCME. Cool.  \\nStudent : [Inaudible].  \\nInstructor (Andrew Ng) : Civi and what else?  \\nStudent : [Inaudible]  \\nInstructor (Andrew Ng) : Synthesis, [inaudible] systems. Yeah, cool.  \\nStudent : Chemi.  \\nInstructor (Andrew Ng) : Chemi. Cool.  \\nStudent : [Inaudible].  \\nInstructor (Andrew Ng) : Aero/astro. Yes, right. Yeah, okay, cool. Anyone else?  \\nStudent : [Inaudible].  \\nInstructor (Andrew Ng) : Pardon? MSNE. All ri ght. Cool. Yeah.  \\nStudent : [Inaudible].  \\nInstructor (Andrew Ng) : Pardon?  \\nStudent : [Inaudible].  \\nInstructor (Andrew Ng) : Endo —  \\nStudent : [Inaudible].  \\nInstructor (Andrew Ng) : Oh, I see, industry. Okay. Cool. Great, great. So as you can \\ntell from a cross-section of th is class, I think we're a very diverse audience in this room, \\nand that's one of the things that makes this class fun to teach and fun to be in, I think.\\n\\nmiddle of class, but because there won't be video you can safely sit there and make faces \\nat me, and that won't show, okay?  \\nLet's see. I also handed out this — ther e were two handouts I hope most of you have, \\ncourse information handout. So let me just sa y a few words about parts of these. On the \\nthird page, there's a section that says Online Resources.  \\nOh, okay. Louder? Actually, could you turn up the volume? Testing. Is this better? \\nTesting, testing. Okay, cool. Thanks.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain > 4:chain:LLMChain > 5:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say \\\"thanks for asking!\\\" at the end of the answer. \\nstatistics for a while or maybe algebra, we'll go over those in the discussion sections as a \\nrefresher for those of you that want one.  \\nLater in this quarter, we'll also use the disc ussion sections to go over extensions for the \\nmaterial that I'm teaching in the main lectur es. So machine learning is a huge field, and \\nthere are a few extensions that we really want  to teach but didn't have time in the main \\nlectures for.\\n\\nof this class will not be very program ming intensive, although we will do some \\nprogramming, mostly in either MATLAB or Octa ve. I'll say a bit more about that later.  \\nI also assume familiarity with basic proba bility and statistics. So most undergraduate \\nstatistics class, like Stat 116 taught here at Stanford, will be more than enough. I'm gonna \\nassume all of you know what ra ndom variables are, that all of you know what expectation \\nis, what a variance or a random variable is. And in case of some of you, it's been a while \\nsince you've seen some of this material. At some of the discussion sections, we'll actually \\ngo over some of the prerequisites, sort of as  a refresher course under prerequisite class. \\nI'll say a bit more about that later as well.  \\nLastly, I also assume familiarity with basi c linear algebra. And again, most undergraduate \\nlinear algebra courses are more than enough. So if you've taken courses like Math 51, \\n103, Math 113 or CS205 at Stanford, that would be more than enough. Basically, I'm \\ngonna assume that all of you know what matrix es and vectors are, that you know how to \\nmultiply matrices and vectors and multiply matrix and matrices, that you know what a matrix inverse is. If you know what an eigenvect or of a matrix is, that'd be even better. \\nBut if you don't quite know or if you're not qu ite sure, that's fine, too. We'll go over it in \\nthe review sections.\\n\\nmany biologers are there here? Wow, just a few, not many. I'm surprised. Anyone from \\nstatistics? Okay, a few. So where are the rest of you from?  \\nStudent : iCME.  \\nInstructor (Andrew Ng) : Say again?  \\nStudent : iCME.  \\nInstructor (Andrew Ng) : iCME. Cool.  \\nStudent : [Inaudible].  \\nInstructor (Andrew Ng) : Civi and what else?  \\nStudent : [Inaudible]  \\nInstructor (Andrew Ng) : Synthesis, [inaudible] systems. Yeah, cool.  \\nStudent : Chemi.  \\nInstructor (Andrew Ng) : Chemi. Cool.  \\nStudent : [Inaudible].  \\nInstructor (Andrew Ng) : Aero/astro. Yes, right. Yeah, okay, cool. Anyone else?  \\nStudent : [Inaudible].  \\nInstructor (Andrew Ng) : Pardon? MSNE. All ri ght. Cool. Yeah.  \\nStudent : [Inaudible].  \\nInstructor (Andrew Ng) : Pardon?  \\nStudent : [Inaudible].  \\nInstructor (Andrew Ng) : Endo —  \\nStudent : [Inaudible].  \\nInstructor (Andrew Ng) : Oh, I see, industry. Okay. Cool. Great, great. So as you can \\ntell from a cross-section of th is class, I think we're a very diverse audience in this room, \\nand that's one of the things that makes this class fun to teach and fun to be in, I think.\\n\\nmiddle of class, but because there won't be video you can safely sit there and make faces \\nat me, and that won't show, okay?  \\nLet's see. I also handed out this — ther e were two handouts I hope most of you have, \\ncourse information handout. So let me just sa y a few words about parts of these. On the \\nthird page, there's a section that says Online Resources.  \\nOh, okay. Louder? Actually, could you turn up the volume? Testing. Is this better? \\nTesting, testing. Okay, cool. Thanks.\\nQuestion: Is statistics a class topic?\\nHelpful Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain > 4:chain:LLMChain > 5:llm:ChatOpenAI] [1.17s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Yes, statistics is a class topic. Thanks for asking!\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"Yes, statistics is a class topic. Thanks for asking!\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 977,\n",
      "      \"completion_tokens\": 12,\n",
      "      \"total_tokens\": 989\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain > 4:chain:LLMChain] [1.17s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"Yes, statistics is a class topic. Thanks for asking!\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain] [1.18s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output_text\": \"Yes, statistics is a class topic. Thanks for asking!\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA] [1.82s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n"
     ]
    }
   ],
   "source": [
    "result = qa_chain({\"query\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'Is statistics a class topic?', 'result': 'Yes, statistics is a class topic. Thanks for asking!', 'source_documents': [Document(page_content=\"statistics for a while or maybe algebra, we'll go over those in the discussion sections as a \\nrefresher for those of you that want one.  \\nLater in this quarter, we'll also use the disc ussion sections to go over extensions for the \\nmaterial that I'm teaching in the main lectur es. So machine learning is a huge field, and \\nthere are a few extensions that we really want  to teach but didn't have time in the main \\nlectures for.\", metadata={'page': 8, 'source': 'ml_doc1.pdf'}), Document(page_content=\"of this class will not be very program ming intensive, although we will do some \\nprogramming, mostly in either MATLAB or Octa ve. I'll say a bit more about that later.  \\nI also assume familiarity with basic proba bility and statistics. So most undergraduate \\nstatistics class, like Stat 116 taught here at Stanford, will be more than enough. I'm gonna \\nassume all of you know what ra ndom variables are, that all of you know what expectation \\nis, what a variance or a random variable is. And in case of some of you, it's been a while \\nsince you've seen some of this material. At some of the discussion sections, we'll actually \\ngo over some of the prerequisites, sort of as  a refresher course under prerequisite class. \\nI'll say a bit more about that later as well.  \\nLastly, I also assume familiarity with basi c linear algebra. And again, most undergraduate \\nlinear algebra courses are more than enough. So if you've taken courses like Math 51, \\n103, Math 113 or CS205 at Stanford, that would be more than enough. Basically, I'm \\ngonna assume that all of you know what matrix es and vectors are, that you know how to \\nmultiply matrices and vectors and multiply matrix and matrices, that you know what a matrix inverse is. If you know what an eigenvect or of a matrix is, that'd be even better. \\nBut if you don't quite know or if you're not qu ite sure, that's fine, too. We'll go over it in \\nthe review sections.\", metadata={'page': 4, 'source': 'ml_doc1.pdf'}), Document(page_content=\"many biologers are there here? Wow, just a few, not many. I'm surprised. Anyone from \\nstatistics? Okay, a few. So where are the rest of you from?  \\nStudent : iCME.  \\nInstructor (Andrew Ng) : Say again?  \\nStudent : iCME.  \\nInstructor (Andrew Ng) : iCME. Cool.  \\nStudent : [Inaudible].  \\nInstructor (Andrew Ng) : Civi and what else?  \\nStudent : [Inaudible]  \\nInstructor (Andrew Ng) : Synthesis, [inaudible] systems. Yeah, cool.  \\nStudent : Chemi.  \\nInstructor (Andrew Ng) : Chemi. Cool.  \\nStudent : [Inaudible].  \\nInstructor (Andrew Ng) : Aero/astro. Yes, right. Yeah, okay, cool. Anyone else?  \\nStudent : [Inaudible].  \\nInstructor (Andrew Ng) : Pardon? MSNE. All ri ght. Cool. Yeah.  \\nStudent : [Inaudible].  \\nInstructor (Andrew Ng) : Pardon?  \\nStudent : [Inaudible].  \\nInstructor (Andrew Ng) : Endo —  \\nStudent : [Inaudible].  \\nInstructor (Andrew Ng) : Oh, I see, industry. Okay. Cool. Great, great. So as you can \\ntell from a cross-section of th is class, I think we're a very diverse audience in this room, \\nand that's one of the things that makes this class fun to teach and fun to be in, I think.\", metadata={'page': 1, 'source': 'ml_doc1.pdf'}), Document(page_content=\"middle of class, but because there won't be video you can safely sit there and make faces \\nat me, and that won't show, okay?  \\nLet's see. I also handed out this — ther e were two handouts I hope most of you have, \\ncourse information handout. So let me just sa y a few words about parts of these. On the \\nthird page, there's a section that says Online Resources.  \\nOh, okay. Louder? Actually, could you turn up the volume? Testing. Is this better? \\nTesting, testing. Okay, cool. Thanks.\", metadata={'page': 4, 'source': 'ml_doc1.pdf'})]}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistics for a while or maybe algebra, we'll go over those in the discussion sections as a \n",
      "refresher for those of you that want one.  \n",
      "Later in this quarter, we'll also use the disc ussion sections to go over extensions for the \n",
      "material that I'm teaching in the main lectur es. So machine learning is a huge field, and \n",
      "there are a few extensions that we really want  to teach but didn't have time in the main \n",
      "lectures for. {'page': 8, 'source': 'ml_doc1.pdf'}\n",
      "of this class will not be very program ming intensive, although we will do some \n",
      "programming, mostly in either MATLAB or Octa ve. I'll say a bit more about that later.  \n",
      "I also assume familiarity with basic proba bility and statistics. So most undergraduate \n",
      "statistics class, like Stat 116 taught here at Stanford, will be more than enough. I'm gonna \n",
      "assume all of you know what ra ndom variables are, that all of you know what expectation \n",
      "is, what a variance or a random variable is. And in case of some of you, it's been a while \n",
      "since you've seen some of this material. At some of the discussion sections, we'll actually \n",
      "go over some of the prerequisites, sort of as  a refresher course under prerequisite class. \n",
      "I'll say a bit more about that later as well.  \n",
      "Lastly, I also assume familiarity with basi c linear algebra. And again, most undergraduate \n",
      "linear algebra courses are more than enough. So if you've taken courses like Math 51, \n",
      "103, Math 113 or CS205 at Stanford, that would be more than enough. Basically, I'm \n",
      "gonna assume that all of you know what matrix es and vectors are, that you know how to \n",
      "multiply matrices and vectors and multiply matrix and matrices, that you know what a matrix inverse is. If you know what an eigenvect or of a matrix is, that'd be even better. \n",
      "But if you don't quite know or if you're not qu ite sure, that's fine, too. We'll go over it in \n",
      "the review sections. {'page': 4, 'source': 'ml_doc1.pdf'}\n",
      "many biologers are there here? Wow, just a few, not many. I'm surprised. Anyone from \n",
      "statistics? Okay, a few. So where are the rest of you from?  \n",
      "Student : iCME.  \n",
      "Instructor (Andrew Ng) : Say again?  \n",
      "Student : iCME.  \n",
      "Instructor (Andrew Ng) : iCME. Cool.  \n",
      "Student : [Inaudible].  \n",
      "Instructor (Andrew Ng) : Civi and what else?  \n",
      "Student : [Inaudible]  \n",
      "Instructor (Andrew Ng) : Synthesis, [inaudible] systems. Yeah, cool.  \n",
      "Student : Chemi.  \n",
      "Instructor (Andrew Ng) : Chemi. Cool.  \n",
      "Student : [Inaudible].  \n",
      "Instructor (Andrew Ng) : Aero/astro. Yes, right. Yeah, okay, cool. Anyone else?  \n",
      "Student : [Inaudible].  \n",
      "Instructor (Andrew Ng) : Pardon? MSNE. All ri ght. Cool. Yeah.  \n",
      "Student : [Inaudible].  \n",
      "Instructor (Andrew Ng) : Pardon?  \n",
      "Student : [Inaudible].  \n",
      "Instructor (Andrew Ng) : Endo —  \n",
      "Student : [Inaudible].  \n",
      "Instructor (Andrew Ng) : Oh, I see, industry. Okay. Cool. Great, great. So as you can \n",
      "tell from a cross-section of th is class, I think we're a very diverse audience in this room, \n",
      "and that's one of the things that makes this class fun to teach and fun to be in, I think. {'page': 1, 'source': 'ml_doc1.pdf'}\n",
      "middle of class, but because there won't be video you can safely sit there and make faces \n",
      "at me, and that won't show, okay?  \n",
      "Let's see. I also handed out this — ther e were two handouts I hope most of you have, \n",
      "course information handout. So let me just sa y a few words about parts of these. On the \n",
      "third page, there's a section that says Online Resources.  \n",
      "Oh, okay. Louder? Actually, could you turn up the volume? Testing. Is this better? \n",
      "Testing, testing. Okay, cool. Thanks. {'page': 4, 'source': 'ml_doc1.pdf'}\n"
     ]
    }
   ],
   "source": [
    "for source in result['source_documents']:\n",
    "    print(source.page_content , source.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"query\": \"Is probability a class topic?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:MapReduceDocumentsChain] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:MapReduceDocumentsChain > 4:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input_list\": [\n",
      "    {\n",
      "      \"context\": \"of this class will not be very program ming intensive, although we will do some \\nprogramming, mostly in either MATLAB or Octa ve. I'll say a bit more about that later.  \\nI also assume familiarity with basic proba bility and statistics. So most undergraduate \\nstatistics class, like Stat 116 taught here at Stanford, will be more than enough. I'm gonna \\nassume all of you know what ra ndom variables are, that all of you know what expectation \\nis, what a variance or a random variable is. And in case of some of you, it's been a while \\nsince you've seen some of this material. At some of the discussion sections, we'll actually \\ngo over some of the prerequisites, sort of as  a refresher course under prerequisite class. \\nI'll say a bit more about that later as well.  \\nLastly, I also assume familiarity with basi c linear algebra. And again, most undergraduate \\nlinear algebra courses are more than enough. So if you've taken courses like Math 51, \\n103, Math 113 or CS205 at Stanford, that would be more than enough. Basically, I'm \\ngonna assume that all of you know what matrix es and vectors are, that you know how to \\nmultiply matrices and vectors and multiply matrix and matrices, that you know what a matrix inverse is. If you know what an eigenvect or of a matrix is, that'd be even better. \\nBut if you don't quite know or if you're not qu ite sure, that's fine, too. We'll go over it in \\nthe review sections.\",\n",
      "      \"question\": \"Is probability a class topic?\"\n",
      "    },\n",
      "    {\n",
      "      \"context\": \"of this class will not be very program ming intensive, although we will do some \\nprogramming, mostly in either MATLAB or Octa ve. I'll say a bit more about that later.  \\nI also assume familiarity with basic proba bility and statistics. So most undergraduate \\nstatistics class, like Stat 116 taught here at Stanford, will be more than enough. I'm gonna \\nassume all of you know what ra ndom variables are, that all of you know what expectation \\nis, what a variance or a random variable is. And in case of some of you, it's been a while \\nsince you've seen some of this material. At some of the discussion sections, we'll actually \\ngo over some of the prerequisites, sort of as  a refresher course under prerequisite class. \\nI'll say a bit more about that later as well.  \\nLastly, I also assume familiarity with basi c linear algebra. And again, most undergraduate \\nlinear algebra courses are more than enough. So if you've taken courses like Math 51, \\n103, Math 113 or CS205 at Stanford, that would be more than enough. Basically, I'm \\ngonna assume that all of you know what matrix es and vectors are, that you know how to \\nmultiply matrices and vectors and multiply matrix and matrices, that you know what a matrix inverse is. If you know what an eigenvect or of a matrix is, that'd be even better. \\nBut if you don't quite know or if you're not qu ite sure, that's fine, too. We'll go over it in \\nthe review sections.\",\n",
      "      \"question\": \"Is probability a class topic?\"\n",
      "    },\n",
      "    {\n",
      "      \"context\": \"Instructor (Andrew Ng) :Yeah, yeah. I mean, you’re asking about overfitting, whether \\nthis is a good model. I thi nk let’s – the thing’s you’re mentioning are maybe deeper \\nquestions about learning algorithms  that we’ll just come back to later, so don’t really \\nwant to get into that right now. Any more questions? Okay.  \\nSo this endows linear regression with a proba bilistic interpretati on. I’m actually going to \\nuse this probabil – use this, sort of, probabilist ic interpretation in order to derive our next \\nlearning algorithm, which will be our first classification algorithm. Okay? So you’ll recall \\nthat I said that regression problems are where the variable Y that you’re trying to predict \\nis continuous values. Now I’m actually gonna ta lk about our first cl assification problem, \\nwhere the value Y you’re trying to predict will be discreet value. You can take on only a \\nsmall number of discrete values and in th is case I’ll talk about binding classification \\nwhere Y takes on only two values, right? So you  come up with classi fication problems if \\nyou’re trying to do, say, a medical diagnosis and try to decide based on some features that \\nthe patient has a disease or does not have a di sease. Or if in the housing example, maybe \\nyou’re trying to decide will this house sell in the next six months or not and the answer is \\neither yes or no. It’ll either be  sold in the next six months or it won’t be. Other standing\",\n",
      "      \"question\": \"Is probability a class topic?\"\n",
      "    },\n",
      "    {\n",
      "      \"context\": \"statistics for a while or maybe algebra, we'll go over those in the discussion sections as a \\nrefresher for those of you that want one.  \\nLater in this quarter, we'll also use the disc ussion sections to go over extensions for the \\nmaterial that I'm teaching in the main lectur es. So machine learning is a huge field, and \\nthere are a few extensions that we really want  to teach but didn't have time in the main \\nlectures for.\",\n",
      "      \"question\": \"Is probability a class topic?\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:MapReduceDocumentsChain > 4:chain:LLMChain > 5:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: Use the following portion of a long document to see if any of the text is relevant to answer the question. \\nReturn any relevant text verbatim.\\n______________________\\nof this class will not be very program ming intensive, although we will do some \\nprogramming, mostly in either MATLAB or Octa ve. I'll say a bit more about that later.  \\nI also assume familiarity with basic proba bility and statistics. So most undergraduate \\nstatistics class, like Stat 116 taught here at Stanford, will be more than enough. I'm gonna \\nassume all of you know what ra ndom variables are, that all of you know what expectation \\nis, what a variance or a random variable is. And in case of some of you, it's been a while \\nsince you've seen some of this material. At some of the discussion sections, we'll actually \\ngo over some of the prerequisites, sort of as  a refresher course under prerequisite class. \\nI'll say a bit more about that later as well.  \\nLastly, I also assume familiarity with basi c linear algebra. And again, most undergraduate \\nlinear algebra courses are more than enough. So if you've taken courses like Math 51, \\n103, Math 113 or CS205 at Stanford, that would be more than enough. Basically, I'm \\ngonna assume that all of you know what matrix es and vectors are, that you know how to \\nmultiply matrices and vectors and multiply matrix and matrices, that you know what a matrix inverse is. If you know what an eigenvect or of a matrix is, that'd be even better. \\nBut if you don't quite know or if you're not qu ite sure, that's fine, too. We'll go over it in \\nthe review sections.\\nHuman: Is probability a class topic?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:MapReduceDocumentsChain > 4:chain:LLMChain > 6:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: Use the following portion of a long document to see if any of the text is relevant to answer the question. \\nReturn any relevant text verbatim.\\n______________________\\nof this class will not be very program ming intensive, although we will do some \\nprogramming, mostly in either MATLAB or Octa ve. I'll say a bit more about that later.  \\nI also assume familiarity with basic proba bility and statistics. So most undergraduate \\nstatistics class, like Stat 116 taught here at Stanford, will be more than enough. I'm gonna \\nassume all of you know what ra ndom variables are, that all of you know what expectation \\nis, what a variance or a random variable is. And in case of some of you, it's been a while \\nsince you've seen some of this material. At some of the discussion sections, we'll actually \\ngo over some of the prerequisites, sort of as  a refresher course under prerequisite class. \\nI'll say a bit more about that later as well.  \\nLastly, I also assume familiarity with basi c linear algebra. And again, most undergraduate \\nlinear algebra courses are more than enough. So if you've taken courses like Math 51, \\n103, Math 113 or CS205 at Stanford, that would be more than enough. Basically, I'm \\ngonna assume that all of you know what matrix es and vectors are, that you know how to \\nmultiply matrices and vectors and multiply matrix and matrices, that you know what a matrix inverse is. If you know what an eigenvect or of a matrix is, that'd be even better. \\nBut if you don't quite know or if you're not qu ite sure, that's fine, too. We'll go over it in \\nthe review sections.\\nHuman: Is probability a class topic?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:MapReduceDocumentsChain > 4:chain:LLMChain > 7:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: Use the following portion of a long document to see if any of the text is relevant to answer the question. \\nReturn any relevant text verbatim.\\n______________________\\nInstructor (Andrew Ng) :Yeah, yeah. I mean, you’re asking about overfitting, whether \\nthis is a good model. I thi nk let’s – the thing’s you’re mentioning are maybe deeper \\nquestions about learning algorithms  that we’ll just come back to later, so don’t really \\nwant to get into that right now. Any more questions? Okay.  \\nSo this endows linear regression with a proba bilistic interpretati on. I’m actually going to \\nuse this probabil – use this, sort of, probabilist ic interpretation in order to derive our next \\nlearning algorithm, which will be our first classification algorithm. Okay? So you’ll recall \\nthat I said that regression problems are where the variable Y that you’re trying to predict \\nis continuous values. Now I’m actually gonna ta lk about our first cl assification problem, \\nwhere the value Y you’re trying to predict will be discreet value. You can take on only a \\nsmall number of discrete values and in th is case I’ll talk about binding classification \\nwhere Y takes on only two values, right? So you  come up with classi fication problems if \\nyou’re trying to do, say, a medical diagnosis and try to decide based on some features that \\nthe patient has a disease or does not have a di sease. Or if in the housing example, maybe \\nyou’re trying to decide will this house sell in the next six months or not and the answer is \\neither yes or no. It’ll either be  sold in the next six months or it won’t be. Other standing\\nHuman: Is probability a class topic?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:MapReduceDocumentsChain > 4:chain:LLMChain > 8:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: Use the following portion of a long document to see if any of the text is relevant to answer the question. \\nReturn any relevant text verbatim.\\n______________________\\nstatistics for a while or maybe algebra, we'll go over those in the discussion sections as a \\nrefresher for those of you that want one.  \\nLater in this quarter, we'll also use the disc ussion sections to go over extensions for the \\nmaterial that I'm teaching in the main lectur es. So machine learning is a huge field, and \\nthere are a few extensions that we really want  to teach but didn't have time in the main \\nlectures for.\\nHuman: Is probability a class topic?\"\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-Ipkgjb8eV212suWndEgA3XmD on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-Ipkgjb8eV212suWndEgA3XmD on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-Ipkgjb8eV212suWndEgA3XmD on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-Ipkgjb8eV212suWndEgA3XmD on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:MapReduceDocumentsChain > 4:chain:LLMChain > 5:llm:ChatOpenAI] [28.16s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Yes, probability is mentioned as a prerequisite for the class. The instructor assumes familiarity with basic probability and statistics.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"Yes, probability is mentioned as a prerequisite for the class. The instructor assumes familiarity with basic probability and statistics.\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 383,\n",
      "      \"completion_tokens\": 22,\n",
      "      \"total_tokens\": 405\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:MapReduceDocumentsChain > 4:chain:LLMChain > 6:llm:ChatOpenAI] [28.16s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Yes, probability is mentioned as a prerequisite for the class. The instructor assumes familiarity with basic probability and statistics.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"Yes, probability is mentioned as a prerequisite for the class. The instructor assumes familiarity with basic probability and statistics.\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 383,\n",
      "      \"completion_tokens\": 22,\n",
      "      \"total_tokens\": 405\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:MapReduceDocumentsChain > 4:chain:LLMChain > 7:llm:ChatOpenAI] [28.16s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Instructor (Andrew Ng): So this endows linear regression with a probabilistic interpretation. I'm actually going to use this probabilistic interpretation in order to derive our next learning algorithm, which will be our first classification algorithm.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"Instructor (Andrew Ng): So this endows linear regression with a probabilistic interpretation. I'm actually going to use this probabilistic interpretation in order to derive our next learning algorithm, which will be our first classification algorithm.\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 379,\n",
      "      \"completion_tokens\": 45,\n",
      "      \"total_tokens\": 424\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:MapReduceDocumentsChain > 4:chain:LLMChain > 8:llm:ChatOpenAI] [28.16s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"No, probability is not mentioned in the provided portion of the document.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"No, probability is not mentioned in the provided portion of the document.\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 150,\n",
      "      \"completion_tokens\": 14,\n",
      "      \"total_tokens\": 164\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:MapReduceDocumentsChain > 4:chain:LLMChain] [28.16s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"outputs\": [\n",
      "    {\n",
      "      \"text\": \"Yes, probability is mentioned as a prerequisite for the class. The instructor assumes familiarity with basic probability and statistics.\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"Yes, probability is mentioned as a prerequisite for the class. The instructor assumes familiarity with basic probability and statistics.\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"Instructor (Andrew Ng): So this endows linear regression with a probabilistic interpretation. I'm actually going to use this probabilistic interpretation in order to derive our next learning algorithm, which will be our first classification algorithm.\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"No, probability is not mentioned in the provided portion of the document.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:MapReduceDocumentsChain > 9:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Is probability a class topic?\",\n",
      "  \"summaries\": \"Yes, probability is mentioned as a prerequisite for the class. The instructor assumes familiarity with basic probability and statistics.\\n\\nYes, probability is mentioned as a prerequisite for the class. The instructor assumes familiarity with basic probability and statistics.\\n\\nInstructor (Andrew Ng): So this endows linear regression with a probabilistic interpretation. I'm actually going to use this probabilistic interpretation in order to derive our next learning algorithm, which will be our first classification algorithm.\\n\\nNo, probability is not mentioned in the provided portion of the document.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:MapReduceDocumentsChain > 9:chain:LLMChain > 10:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: Given the following extracted parts of a long document and a question, create a final answer. \\nIf you don't know the answer, just say that you don't know. Don't try to make up an answer.\\n______________________\\nYes, probability is mentioned as a prerequisite for the class. The instructor assumes familiarity with basic probability and statistics.\\n\\nYes, probability is mentioned as a prerequisite for the class. The instructor assumes familiarity with basic probability and statistics.\\n\\nInstructor (Andrew Ng): So this endows linear regression with a probabilistic interpretation. I'm actually going to use this probabilistic interpretation in order to derive our next learning algorithm, which will be our first classification algorithm.\\n\\nNo, probability is not mentioned in the provided portion of the document.\\nHuman: Is probability a class topic?\"\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-Ipkgjb8eV212suWndEgA3XmD on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-Ipkgjb8eV212suWndEgA3XmD on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-Ipkgjb8eV212suWndEgA3XmD on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:MapReduceDocumentsChain > 9:chain:LLMChain > 10:llm:ChatOpenAI] [14.53s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Based on the provided information, it is not clear whether probability is a specific topic in the class.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"Based on the provided information, it is not clear whether probability is a specific topic in the class.\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 167,\n",
      "      \"completion_tokens\": 20,\n",
      "      \"total_tokens\": 187\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:MapReduceDocumentsChain > 9:chain:LLMChain] [14.53s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"Based on the provided information, it is not clear whether probability is a specific topic in the class.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:MapReduceDocumentsChain] [42.70s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output_text\": \"Based on the provided information, it is not clear whether probability is a specific topic in the class.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA] [43.29s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"result\": \"Based on the provided information, it is not clear whether probability is a specific topic in the class.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "question = \"Is probability a class topic?\"\n",
    "qa_chain_mr = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    chain_type=\"map_reduce\"\n",
    ")\n",
    "\n",
    "result = qa_chain_mr({\"query\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieval Limitation : can't preserve memory/history of chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"query\": \"Is probability a class topic?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain > 4:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Is probability a class topic?\",\n",
      "  \"context\": \"of this class will not be very program ming intensive, although we will do some \\nprogramming, mostly in either MATLAB or Octa ve. I'll say a bit more about that later.  \\nI also assume familiarity with basic proba bility and statistics. So most undergraduate \\nstatistics class, like Stat 116 taught here at Stanford, will be more than enough. I'm gonna \\nassume all of you know what ra ndom variables are, that all of you know what expectation \\nis, what a variance or a random variable is. And in case of some of you, it's been a while \\nsince you've seen some of this material. At some of the discussion sections, we'll actually \\ngo over some of the prerequisites, sort of as  a refresher course under prerequisite class. \\nI'll say a bit more about that later as well.  \\nLastly, I also assume familiarity with basi c linear algebra. And again, most undergraduate \\nlinear algebra courses are more than enough. So if you've taken courses like Math 51, \\n103, Math 113 or CS205 at Stanford, that would be more than enough. Basically, I'm \\ngonna assume that all of you know what matrix es and vectors are, that you know how to \\nmultiply matrices and vectors and multiply matrix and matrices, that you know what a matrix inverse is. If you know what an eigenvect or of a matrix is, that'd be even better. \\nBut if you don't quite know or if you're not qu ite sure, that's fine, too. We'll go over it in \\nthe review sections.\\n\\nof this class will not be very program ming intensive, although we will do some \\nprogramming, mostly in either MATLAB or Octa ve. I'll say a bit more about that later.  \\nI also assume familiarity with basic proba bility and statistics. So most undergraduate \\nstatistics class, like Stat 116 taught here at Stanford, will be more than enough. I'm gonna \\nassume all of you know what ra ndom variables are, that all of you know what expectation \\nis, what a variance or a random variable is. And in case of some of you, it's been a while \\nsince you've seen some of this material. At some of the discussion sections, we'll actually \\ngo over some of the prerequisites, sort of as  a refresher course under prerequisite class. \\nI'll say a bit more about that later as well.  \\nLastly, I also assume familiarity with basi c linear algebra. And again, most undergraduate \\nlinear algebra courses are more than enough. So if you've taken courses like Math 51, \\n103, Math 113 or CS205 at Stanford, that would be more than enough. Basically, I'm \\ngonna assume that all of you know what matrix es and vectors are, that you know how to \\nmultiply matrices and vectors and multiply matrix and matrices, that you know what a matrix inverse is. If you know what an eigenvect or of a matrix is, that'd be even better. \\nBut if you don't quite know or if you're not qu ite sure, that's fine, too. We'll go over it in \\nthe review sections.\\n\\nInstructor (Andrew Ng) :Yeah, yeah. I mean, you’re asking about overfitting, whether \\nthis is a good model. I thi nk let’s – the thing’s you’re mentioning are maybe deeper \\nquestions about learning algorithms  that we’ll just come back to later, so don’t really \\nwant to get into that right now. Any more questions? Okay.  \\nSo this endows linear regression with a proba bilistic interpretati on. I’m actually going to \\nuse this probabil – use this, sort of, probabilist ic interpretation in order to derive our next \\nlearning algorithm, which will be our first classification algorithm. Okay? So you’ll recall \\nthat I said that regression problems are where the variable Y that you’re trying to predict \\nis continuous values. Now I’m actually gonna ta lk about our first cl assification problem, \\nwhere the value Y you’re trying to predict will be discreet value. You can take on only a \\nsmall number of discrete values and in th is case I’ll talk about binding classification \\nwhere Y takes on only two values, right? So you  come up with classi fication problems if \\nyou’re trying to do, say, a medical diagnosis and try to decide based on some features that \\nthe patient has a disease or does not have a di sease. Or if in the housing example, maybe \\nyou’re trying to decide will this house sell in the next six months or not and the answer is \\neither yes or no. It’ll either be  sold in the next six months or it won’t be. Other standing\\n\\nstatistics for a while or maybe algebra, we'll go over those in the discussion sections as a \\nrefresher for those of you that want one.  \\nLater in this quarter, we'll also use the disc ussion sections to go over extensions for the \\nmaterial that I'm teaching in the main lectur es. So machine learning is a huge field, and \\nthere are a few extensions that we really want  to teach but didn't have time in the main \\nlectures for.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain > 4:chain:LLMChain > 5:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: Use the following pieces of context to answer the users question. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\nof this class will not be very program ming intensive, although we will do some \\nprogramming, mostly in either MATLAB or Octa ve. I'll say a bit more about that later.  \\nI also assume familiarity with basic proba bility and statistics. So most undergraduate \\nstatistics class, like Stat 116 taught here at Stanford, will be more than enough. I'm gonna \\nassume all of you know what ra ndom variables are, that all of you know what expectation \\nis, what a variance or a random variable is. And in case of some of you, it's been a while \\nsince you've seen some of this material. At some of the discussion sections, we'll actually \\ngo over some of the prerequisites, sort of as  a refresher course under prerequisite class. \\nI'll say a bit more about that later as well.  \\nLastly, I also assume familiarity with basi c linear algebra. And again, most undergraduate \\nlinear algebra courses are more than enough. So if you've taken courses like Math 51, \\n103, Math 113 or CS205 at Stanford, that would be more than enough. Basically, I'm \\ngonna assume that all of you know what matrix es and vectors are, that you know how to \\nmultiply matrices and vectors and multiply matrix and matrices, that you know what a matrix inverse is. If you know what an eigenvect or of a matrix is, that'd be even better. \\nBut if you don't quite know or if you're not qu ite sure, that's fine, too. We'll go over it in \\nthe review sections.\\n\\nof this class will not be very program ming intensive, although we will do some \\nprogramming, mostly in either MATLAB or Octa ve. I'll say a bit more about that later.  \\nI also assume familiarity with basic proba bility and statistics. So most undergraduate \\nstatistics class, like Stat 116 taught here at Stanford, will be more than enough. I'm gonna \\nassume all of you know what ra ndom variables are, that all of you know what expectation \\nis, what a variance or a random variable is. And in case of some of you, it's been a while \\nsince you've seen some of this material. At some of the discussion sections, we'll actually \\ngo over some of the prerequisites, sort of as  a refresher course under prerequisite class. \\nI'll say a bit more about that later as well.  \\nLastly, I also assume familiarity with basi c linear algebra. And again, most undergraduate \\nlinear algebra courses are more than enough. So if you've taken courses like Math 51, \\n103, Math 113 or CS205 at Stanford, that would be more than enough. Basically, I'm \\ngonna assume that all of you know what matrix es and vectors are, that you know how to \\nmultiply matrices and vectors and multiply matrix and matrices, that you know what a matrix inverse is. If you know what an eigenvect or of a matrix is, that'd be even better. \\nBut if you don't quite know or if you're not qu ite sure, that's fine, too. We'll go over it in \\nthe review sections.\\n\\nInstructor (Andrew Ng) :Yeah, yeah. I mean, you’re asking about overfitting, whether \\nthis is a good model. I thi nk let’s – the thing’s you’re mentioning are maybe deeper \\nquestions about learning algorithms  that we’ll just come back to later, so don’t really \\nwant to get into that right now. Any more questions? Okay.  \\nSo this endows linear regression with a proba bilistic interpretati on. I’m actually going to \\nuse this probabil – use this, sort of, probabilist ic interpretation in order to derive our next \\nlearning algorithm, which will be our first classification algorithm. Okay? So you’ll recall \\nthat I said that regression problems are where the variable Y that you’re trying to predict \\nis continuous values. Now I’m actually gonna ta lk about our first cl assification problem, \\nwhere the value Y you’re trying to predict will be discreet value. You can take on only a \\nsmall number of discrete values and in th is case I’ll talk about binding classification \\nwhere Y takes on only two values, right? So you  come up with classi fication problems if \\nyou’re trying to do, say, a medical diagnosis and try to decide based on some features that \\nthe patient has a disease or does not have a di sease. Or if in the housing example, maybe \\nyou’re trying to decide will this house sell in the next six months or not and the answer is \\neither yes or no. It’ll either be  sold in the next six months or it won’t be. Other standing\\n\\nstatistics for a while or maybe algebra, we'll go over those in the discussion sections as a \\nrefresher for those of you that want one.  \\nLater in this quarter, we'll also use the disc ussion sections to go over extensions for the \\nmaterial that I'm teaching in the main lectur es. So machine learning is a huge field, and \\nthere are a few extensions that we really want  to teach but didn't have time in the main \\nlectures for.\\nHuman: Is probability a class topic?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain > 4:chain:LLMChain > 5:llm:ChatOpenAI] [2.08s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Yes, probability is a topic that will be covered in this class. The instructor assumes familiarity with basic probability and statistics, so it is expected that students have prior knowledge in this area.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"Yes, probability is a topic that will be covered in this class. The instructor assumes familiarity with basic probability and statistics, so it is expected that students have prior knowledge in this area.\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 1153,\n",
      "      \"completion_tokens\": 37,\n",
      "      \"total_tokens\": 1190\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain > 4:chain:LLMChain] [2.08s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"Yes, probability is a topic that will be covered in this class. The instructor assumes familiarity with basic probability and statistics, so it is expected that students have prior knowledge in this area.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain] [2.08s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output_text\": \"Yes, probability is a topic that will be covered in this class. The instructor assumes familiarity with basic probability and statistics, so it is expected that students have prior knowledge in this area.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA] [3.15s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"result\": \"Yes, probability is a topic that will be covered in this class. The instructor assumes familiarity with basic probability and statistics, so it is expected that students have prior knowledge in this area.\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Yes, probability is a topic that will be covered in this class. The instructor assumes familiarity with basic probability and statistics, so it is expected that students have prior knowledge in this area.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Is probability a class topic?\"\n",
    "result = qa_chain({\"query\": question})\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"query\": \"why are those prerequesites needed?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain > 4:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"why are those prerequesites needed?\",\n",
      "  \"context\": \"So, see, most of us use learning algorithms half a dozen, a dozen, maybe dozens of times \\nwithout even knowing it.  \\nAnd of course, learning algorithms are also  doing things like giving us a growing \\nunderstanding of the human genome. So if so meday we ever find a cure for cancer, I bet \\nlearning algorithms will have had a large role in that. That's sort of the thing that Tom \\nworks on, yes?  \\nSo in teaching this class, I sort of have thre e goals. One of them is just to I hope convey \\nsome of my own excitement a bout machine learning to you.  \\nThe second goal is by the end of this class, I hope all of you will be ab le to apply state-of-\\nthe-art machine learning algorithms to whatev er problems you're interested in. And if you \\never need to build a system for reading zi p codes, you'll know how to do that by the end \\nof this class.  \\nAnd lastly, by the end of this class, I reali ze that only a subset of  you are interested in \\ndoing research in machine learning, but by the c onclusion of this class,  I hope that all of \\nyou will actually be well qualified to star t doing research in machine learning, okay?  \\nSo let's say a few words about logistics. The prerequisites of this class are written on one \\nof the handouts, are as follows: In this class, I'm going to assume that all of you have sort \\nof basic knowledge of computer science and kn owledge of the basic computer skills and \\nprinciples. So I assume all of you know what big?O notation, that all of you know about\\n\\nSo, see, most of us use learning algorithms half a dozen, a dozen, maybe dozens of times \\nwithout even knowing it.  \\nAnd of course, learning algorithms are also  doing things like giving us a growing \\nunderstanding of the human genome. So if so meday we ever find a cure for cancer, I bet \\nlearning algorithms will have had a large role in that. That's sort of the thing that Tom \\nworks on, yes?  \\nSo in teaching this class, I sort of have thre e goals. One of them is just to I hope convey \\nsome of my own excitement a bout machine learning to you.  \\nThe second goal is by the end of this class, I hope all of you will be ab le to apply state-of-\\nthe-art machine learning algorithms to whatev er problems you're interested in. And if you \\never need to build a system for reading zi p codes, you'll know how to do that by the end \\nof this class.  \\nAnd lastly, by the end of this class, I reali ze that only a subset of  you are interested in \\ndoing research in machine learning, but by the c onclusion of this class,  I hope that all of \\nyou will actually be well qualified to star t doing research in machine learning, okay?  \\nSo let's say a few words about logistics. The prerequisites of this class are written on one \\nof the handouts, are as follows: In this class, I'm going to assume that all of you have sort \\nof basic knowledge of computer science and kn owledge of the basic computer skills and \\nprinciples. So I assume all of you know what big?O notation, that all of you know about\\n\\nof this class will not be very program ming intensive, although we will do some \\nprogramming, mostly in either MATLAB or Octa ve. I'll say a bit more about that later.  \\nI also assume familiarity with basic proba bility and statistics. So most undergraduate \\nstatistics class, like Stat 116 taught here at Stanford, will be more than enough. I'm gonna \\nassume all of you know what ra ndom variables are, that all of you know what expectation \\nis, what a variance or a random variable is. And in case of some of you, it's been a while \\nsince you've seen some of this material. At some of the discussion sections, we'll actually \\ngo over some of the prerequisites, sort of as  a refresher course under prerequisite class. \\nI'll say a bit more about that later as well.  \\nLastly, I also assume familiarity with basi c linear algebra. And again, most undergraduate \\nlinear algebra courses are more than enough. So if you've taken courses like Math 51, \\n103, Math 113 or CS205 at Stanford, that would be more than enough. Basically, I'm \\ngonna assume that all of you know what matrix es and vectors are, that you know how to \\nmultiply matrices and vectors and multiply matrix and matrices, that you know what a matrix inverse is. If you know what an eigenvect or of a matrix is, that'd be even better. \\nBut if you don't quite know or if you're not qu ite sure, that's fine, too. We'll go over it in \\nthe review sections.\\n\\nof this class will not be very program ming intensive, although we will do some \\nprogramming, mostly in either MATLAB or Octa ve. I'll say a bit more about that later.  \\nI also assume familiarity with basic proba bility and statistics. So most undergraduate \\nstatistics class, like Stat 116 taught here at Stanford, will be more than enough. I'm gonna \\nassume all of you know what ra ndom variables are, that all of you know what expectation \\nis, what a variance or a random variable is. And in case of some of you, it's been a while \\nsince you've seen some of this material. At some of the discussion sections, we'll actually \\ngo over some of the prerequisites, sort of as  a refresher course under prerequisite class. \\nI'll say a bit more about that later as well.  \\nLastly, I also assume familiarity with basi c linear algebra. And again, most undergraduate \\nlinear algebra courses are more than enough. So if you've taken courses like Math 51, \\n103, Math 113 or CS205 at Stanford, that would be more than enough. Basically, I'm \\ngonna assume that all of you know what matrix es and vectors are, that you know how to \\nmultiply matrices and vectors and multiply matrix and matrices, that you know what a matrix inverse is. If you know what an eigenvect or of a matrix is, that'd be even better. \\nBut if you don't quite know or if you're not qu ite sure, that's fine, too. We'll go over it in \\nthe review sections.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain > 4:chain:LLMChain > 5:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: Use the following pieces of context to answer the users question. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\nSo, see, most of us use learning algorithms half a dozen, a dozen, maybe dozens of times \\nwithout even knowing it.  \\nAnd of course, learning algorithms are also  doing things like giving us a growing \\nunderstanding of the human genome. So if so meday we ever find a cure for cancer, I bet \\nlearning algorithms will have had a large role in that. That's sort of the thing that Tom \\nworks on, yes?  \\nSo in teaching this class, I sort of have thre e goals. One of them is just to I hope convey \\nsome of my own excitement a bout machine learning to you.  \\nThe second goal is by the end of this class, I hope all of you will be ab le to apply state-of-\\nthe-art machine learning algorithms to whatev er problems you're interested in. And if you \\never need to build a system for reading zi p codes, you'll know how to do that by the end \\nof this class.  \\nAnd lastly, by the end of this class, I reali ze that only a subset of  you are interested in \\ndoing research in machine learning, but by the c onclusion of this class,  I hope that all of \\nyou will actually be well qualified to star t doing research in machine learning, okay?  \\nSo let's say a few words about logistics. The prerequisites of this class are written on one \\nof the handouts, are as follows: In this class, I'm going to assume that all of you have sort \\nof basic knowledge of computer science and kn owledge of the basic computer skills and \\nprinciples. So I assume all of you know what big?O notation, that all of you know about\\n\\nSo, see, most of us use learning algorithms half a dozen, a dozen, maybe dozens of times \\nwithout even knowing it.  \\nAnd of course, learning algorithms are also  doing things like giving us a growing \\nunderstanding of the human genome. So if so meday we ever find a cure for cancer, I bet \\nlearning algorithms will have had a large role in that. That's sort of the thing that Tom \\nworks on, yes?  \\nSo in teaching this class, I sort of have thre e goals. One of them is just to I hope convey \\nsome of my own excitement a bout machine learning to you.  \\nThe second goal is by the end of this class, I hope all of you will be ab le to apply state-of-\\nthe-art machine learning algorithms to whatev er problems you're interested in. And if you \\never need to build a system for reading zi p codes, you'll know how to do that by the end \\nof this class.  \\nAnd lastly, by the end of this class, I reali ze that only a subset of  you are interested in \\ndoing research in machine learning, but by the c onclusion of this class,  I hope that all of \\nyou will actually be well qualified to star t doing research in machine learning, okay?  \\nSo let's say a few words about logistics. The prerequisites of this class are written on one \\nof the handouts, are as follows: In this class, I'm going to assume that all of you have sort \\nof basic knowledge of computer science and kn owledge of the basic computer skills and \\nprinciples. So I assume all of you know what big?O notation, that all of you know about\\n\\nof this class will not be very program ming intensive, although we will do some \\nprogramming, mostly in either MATLAB or Octa ve. I'll say a bit more about that later.  \\nI also assume familiarity with basic proba bility and statistics. So most undergraduate \\nstatistics class, like Stat 116 taught here at Stanford, will be more than enough. I'm gonna \\nassume all of you know what ra ndom variables are, that all of you know what expectation \\nis, what a variance or a random variable is. And in case of some of you, it's been a while \\nsince you've seen some of this material. At some of the discussion sections, we'll actually \\ngo over some of the prerequisites, sort of as  a refresher course under prerequisite class. \\nI'll say a bit more about that later as well.  \\nLastly, I also assume familiarity with basi c linear algebra. And again, most undergraduate \\nlinear algebra courses are more than enough. So if you've taken courses like Math 51, \\n103, Math 113 or CS205 at Stanford, that would be more than enough. Basically, I'm \\ngonna assume that all of you know what matrix es and vectors are, that you know how to \\nmultiply matrices and vectors and multiply matrix and matrices, that you know what a matrix inverse is. If you know what an eigenvect or of a matrix is, that'd be even better. \\nBut if you don't quite know or if you're not qu ite sure, that's fine, too. We'll go over it in \\nthe review sections.\\n\\nof this class will not be very program ming intensive, although we will do some \\nprogramming, mostly in either MATLAB or Octa ve. I'll say a bit more about that later.  \\nI also assume familiarity with basic proba bility and statistics. So most undergraduate \\nstatistics class, like Stat 116 taught here at Stanford, will be more than enough. I'm gonna \\nassume all of you know what ra ndom variables are, that all of you know what expectation \\nis, what a variance or a random variable is. And in case of some of you, it's been a while \\nsince you've seen some of this material. At some of the discussion sections, we'll actually \\ngo over some of the prerequisites, sort of as  a refresher course under prerequisite class. \\nI'll say a bit more about that later as well.  \\nLastly, I also assume familiarity with basi c linear algebra. And again, most undergraduate \\nlinear algebra courses are more than enough. So if you've taken courses like Math 51, \\n103, Math 113 or CS205 at Stanford, that would be more than enough. Basically, I'm \\ngonna assume that all of you know what matrix es and vectors are, that you know how to \\nmultiply matrices and vectors and multiply matrix and matrices, that you know what a matrix inverse is. If you know what an eigenvect or of a matrix is, that'd be even better. \\nBut if you don't quite know or if you're not qu ite sure, that's fine, too. We'll go over it in \\nthe review sections.\\nHuman: why are those prerequesites needed?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain > 4:chain:LLMChain > 5:llm:ChatOpenAI] [7.57s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The prerequisites are needed because they provide the foundational knowledge and skills necessary to understand and apply machine learning algorithms. \\n\\nBasic knowledge of computer science and computer skills is important because machine learning algorithms often involve programming and working with data. Understanding concepts like big-O notation helps in analyzing the efficiency and scalability of algorithms.\\n\\nFamiliarity with probability and statistics is necessary because machine learning involves working with data and making predictions based on statistical models. Knowledge of random variables, expectation, variance, and probability distributions is essential in understanding and implementing machine learning algorithms.\\n\\nBasic knowledge of linear algebra is required because many machine learning algorithms involve matrix operations and linear transformations. Understanding concepts like matrices, vectors, matrix multiplication, and matrix inverse is important in working with data and performing calculations in machine learning.\\n\\nOverall, these prerequisites ensure that students have the necessary background knowledge to effectively learn and apply machine learning algorithms.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"The prerequisites are needed because they provide the foundational knowledge and skills necessary to understand and apply machine learning algorithms. \\n\\nBasic knowledge of computer science and computer skills is important because machine learning algorithms often involve programming and working with data. Understanding concepts like big-O notation helps in analyzing the efficiency and scalability of algorithms.\\n\\nFamiliarity with probability and statistics is necessary because machine learning involves working with data and making predictions based on statistical models. Knowledge of random variables, expectation, variance, and probability distributions is essential in understanding and implementing machine learning algorithms.\\n\\nBasic knowledge of linear algebra is required because many machine learning algorithms involve matrix operations and linear transformations. Understanding concepts like matrices, vectors, matrix multiplication, and matrix inverse is important in working with data and performing calculations in machine learning.\\n\\nOverall, these prerequisites ensure that students have the necessary background knowledge to effectively learn and apply machine learning algorithms.\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 1441,\n",
      "      \"completion_tokens\": 172,\n",
      "      \"total_tokens\": 1613\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain > 4:chain:LLMChain] [7.57s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"The prerequisites are needed because they provide the foundational knowledge and skills necessary to understand and apply machine learning algorithms. \\n\\nBasic knowledge of computer science and computer skills is important because machine learning algorithms often involve programming and working with data. Understanding concepts like big-O notation helps in analyzing the efficiency and scalability of algorithms.\\n\\nFamiliarity with probability and statistics is necessary because machine learning involves working with data and making predictions based on statistical models. Knowledge of random variables, expectation, variance, and probability distributions is essential in understanding and implementing machine learning algorithms.\\n\\nBasic knowledge of linear algebra is required because many machine learning algorithms involve matrix operations and linear transformations. Understanding concepts like matrices, vectors, matrix multiplication, and matrix inverse is important in working with data and performing calculations in machine learning.\\n\\nOverall, these prerequisites ensure that students have the necessary background knowledge to effectively learn and apply machine learning algorithms.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain] [7.57s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output_text\": \"The prerequisites are needed because they provide the foundational knowledge and skills necessary to understand and apply machine learning algorithms. \\n\\nBasic knowledge of computer science and computer skills is important because machine learning algorithms often involve programming and working with data. Understanding concepts like big-O notation helps in analyzing the efficiency and scalability of algorithms.\\n\\nFamiliarity with probability and statistics is necessary because machine learning involves working with data and making predictions based on statistical models. Knowledge of random variables, expectation, variance, and probability distributions is essential in understanding and implementing machine learning algorithms.\\n\\nBasic knowledge of linear algebra is required because many machine learning algorithms involve matrix operations and linear transformations. Understanding concepts like matrices, vectors, matrix multiplication, and matrix inverse is important in working with data and performing calculations in machine learning.\\n\\nOverall, these prerequisites ensure that students have the necessary background knowledge to effectively learn and apply machine learning algorithms.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA] [8.34s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"result\": \"The prerequisites are needed because they provide the foundational knowledge and skills necessary to understand and apply machine learning algorithms. \\n\\nBasic knowledge of computer science and computer skills is important because machine learning algorithms often involve programming and working with data. Understanding concepts like big-O notation helps in analyzing the efficiency and scalability of algorithms.\\n\\nFamiliarity with probability and statistics is necessary because machine learning involves working with data and making predictions based on statistical models. Knowledge of random variables, expectation, variance, and probability distributions is essential in understanding and implementing machine learning algorithms.\\n\\nBasic knowledge of linear algebra is required because many machine learning algorithms involve matrix operations and linear transformations. Understanding concepts like matrices, vectors, matrix multiplication, and matrix inverse is important in working with data and performing calculations in machine learning.\\n\\nOverall, these prerequisites ensure that students have the necessary background knowledge to effectively learn and apply machine learning algorithms.\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The prerequisites are needed because they provide the foundational knowledge and skills necessary to understand and apply machine learning algorithms. \\n\\nBasic knowledge of computer science and computer skills is important because machine learning algorithms often involve programming and working with data. Understanding concepts like big-O notation helps in analyzing the efficiency and scalability of algorithms.\\n\\nFamiliarity with probability and statistics is necessary because machine learning involves working with data and making predictions based on statistical models. Knowledge of random variables, expectation, variance, and probability distributions is essential in understanding and implementing machine learning algorithms.\\n\\nBasic knowledge of linear algebra is required because many machine learning algorithms involve matrix operations and linear transformations. Understanding concepts like matrices, vectors, matrix multiplication, and matrix inverse is important in working with data and performing calculations in machine learning.\\n\\nOverall, these prerequisites ensure that students have the necessary background knowledge to effectively learn and apply machine learning algorithms.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"why are those prerequesites needed?\"\n",
    "result = qa_chain({\"query\": question})\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openaivenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
