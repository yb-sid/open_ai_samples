{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat bot for CADTH recommendations report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a global prompt template to be used for Q/A agaist the pdf \n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "PDF_PROMPT = \"\"\"\n",
    "You are the world's best data-analyst. You are provided with a `context` delimited by ```.\n",
    "Following information about the `context` is provided:\n",
    "1. `context` is a combination of extracts from a report.\n",
    "2. this report contains details about a medical drug and a recommendation on it. Note: you are not provided with entire report.\n",
    "3. this recommendation is based on certain evidences within the `context`\n",
    "4. the report is published by an independent company on behalf of a government.\n",
    "\n",
    "You are assigned with following tasks:\n",
    "1. Read and analyse the `context` carefully. Keep in mind all the information provided above about the context.\n",
    "2. You will be provided with a question and need to answer it based solely on `context`\n",
    "3. along with the answer , you need to return a key called as \"evidences\" which contains all the \n",
    "\n",
    "Note: Make sure to not make up answers on your own. If you cannot answer the question ,respond with \"Not enough information available in provided context\"\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import all dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain # to debug if required\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import SpacyTextSplitter # specific for preserving context and meaning\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a method to load PDF and init conversation chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf_into_db(file , chain_type = \"stuff\" , k = 4):\n",
    "    \"\"\"\n",
    "    params:\n",
    "    ---------\n",
    "    :file - `str` path of PDF file to be loaded\n",
    "    :chain_type - `str` technique used to fill up context in a LLM prompt , values can be `stuff` , `map_reduce` , `refine`\n",
    "\n",
    "    :k - number of top chunks to be used for searching in vectorstore\n",
    "    \"\"\"\n",
    "\n",
    "    # load the pdf \n",
    "    loader = PyPDFLoader(file)\n",
    "    docs = loader.load()\n",
    "\n",
    "    # split the documents \n",
    "    text_splitter = SpacyTextSplitter(kwargs={\"chunk_size\":2000, \"chunk_overlap\":200})\n",
    "    doc_chunks = text_splitter.split_documents(documents= docs)\n",
    "\n",
    "    # define embedding \n",
    "    embedding_model = OpenAIEmbeddings()\n",
    "    persist_directory = \"db/chroma\"\n",
    "    # create vector db :: chroma\n",
    "    vectorstore = Chroma.from_documents(\n",
    "          documents = doc_chunks ,\n",
    "          embedding= embedding_model , \n",
    "          persist_directory= persist_directory,\n",
    "    )\n",
    "\n",
    "    # create memory \n",
    "    chat_memory = ConversationBufferMemory(\n",
    "        memory_key=\"chat_history\", # name used inside retrieval chain\n",
    "        return_messages=True\n",
    "    ) # will contain pairs of HumanMessage and AIMessage\n",
    "\n",
    "    chat_model = ChatOpenAI(temperature=0.0)\n",
    "\n",
    "    # define retreival \n",
    "    retriever = vectorstore.as_retriever(search_type = \"mmr\" , search_kwargs= {\"k\":k}) # need to check with \"mmr\"\n",
    "\n",
    "     \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openaivenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
